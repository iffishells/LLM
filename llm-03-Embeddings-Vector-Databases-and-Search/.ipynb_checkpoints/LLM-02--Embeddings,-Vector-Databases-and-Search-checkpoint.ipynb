{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " MAGIC %pip install faiss-cpu==1.7.4 chromadb==0.3.21"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16d4f53c83820f10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample\n",
    "\n",
    "pdf_subset = pdf.head(1000)\n",
    "\n",
    "def example_create_fn(doc1: pd.Series) -> InputExample:\n",
    "    \"\"\"\n",
    "    Helper function that outputs a sentence_transformer guid, label, and text\n",
    "    \"\"\"\n",
    "    return InputExample(texts=[doc1])\n",
    "\n",
    "faiss_train_examples = pdf_subset.apply(\n",
    "    lambda x: example_create_fn(x[\"title\"]), axis=1\n",
    ").tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a66063b81b2eddd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    \"all-MiniLM-L6-v2\", \n",
    "    cache_folder=DA.paths.datasets\n",
    ")  # Use a pre-cached model\n",
    "faiss_title_embedding = model.encode(pdf_subset.title.values.tolist())\n",
    "len(faiss_title_embedding), len(faiss_title_embedding[0])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c7e7bd39c79ed30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC ### Step 3: Saving embedding vectors to FAISS index\n",
    "# MAGIC Below, we create the FAISS index object based on our embedding vectors, normalize vectors, and add these vectors to the FAISS index. \n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "pdf_to_index = pdf_subset.set_index([\"id\"], drop=False)\n",
    "id_index = np.array(pdf_to_index.id.values).flatten().astype(\"int\")\n",
    "\n",
    "content_encoded_normalized = faiss_title_embedding.copy()\n",
    "faiss.normalize_L2(content_encoded_normalized)\n",
    "\n",
    "# Index1DMap translates search results to IDs: https://faiss.ai/cpp_api/file/IndexIDMap_8h.html#_CPPv4I0EN5faiss18IndexIDMapTemplateE\n",
    "# The IndexFlatIP below builds index\n",
    "index_content = faiss.IndexIDMap(faiss.IndexFlatIP(len(faiss_title_embedding[0])))\n",
    "index_content.add_with_ids(content_encoded_normalized, id_index)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce8ff327a5a4bdd5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 4: Search for relevant documents\n",
    "# MAGIC\n",
    "# MAGIC We define a search function below to first vectorize our query text, and then search for the vectors with the closest distance. \n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def search_content(query, pdf_to_index, k=3):\n",
    "    query_vector = model.encode([query])\n",
    "    faiss.normalize_L2(query_vector)\n",
    "\n",
    "    # We set k to limit the number of vectors we want to return\n",
    "    top_k = index_content.search(query_vector, k)\n",
    "    ids = top_k[1][0].tolist()\n",
    "    similarities = top_k[0][0].tolist()\n",
    "    results = pdf_to_index.loc[ids]\n",
    "    results[\"similarities\"] = similarities\n",
    "    return results\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "902e99323b1c548b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC Tada! Now you can query for similar content! Notice that you did not have to configure any database networks beforehand nor pass in any credentials. FAISS works locally with your code.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "display(search_content(\"animal\", pdf_to_index))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "554a9ddb40231f38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC Up until now, we haven't done the last step of conducting Q/A with a language model yet. We are going to demonstrate this with Chroma, a vector database.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Vector Database: Chroma\n",
    "# MAGIC\n",
    "# MAGIC Chroma is an open-source embedding database. The company just raised its [seed funding in April 2023](https://www.trychroma.com/blog/seed) and is quickly becoming popular to support LLM-based applications. \n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "chroma_client = chromadb.Client(\n",
    "    Settings(\n",
    "        chroma_db_impl=\"duckdb+parquet\",\n",
    "        persist_directory=DA.paths.user_db,  # this is an optional argument. If you don't supply this, the data will be ephemeral\n",
    "    )\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1c70af451e4882c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC\n",
    "# MAGIC ### Chroma Concept: Collection\n",
    "# MAGIC\n",
    "# MAGIC Chroma `collection` is akin to an index that stores one set of your documents. \n",
    "# MAGIC\n",
    "# MAGIC According to the [docs](https://docs.trychroma.com/getting-started): \n",
    "# MAGIC > Collections are where you will store your embeddings, documents, and additional metadata\n",
    "# MAGIC\n",
    "# MAGIC The nice thing about ChromaDB is that if you don't supply a model to vectorize text into embeddings, it will automatically load a default embedding function, i.e. `SentenceTransformerEmbeddingFunction`. It can handle tokenization, embedding, and indexing automatically for you. If you would like to change the embedding model, read [here on how to do that](https://docs.trychroma.com/embeddings). TLDR: you can add an optional `model_name` argument. \n",
    "# MAGIC\n",
    "# MAGIC You can read [the documentation here](https://docs.trychroma.com/usage-guide#using-collections) on rules for collection names.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "collection_name = \"my_news\"\n",
    "\n",
    "# If you have created the collection before, you need to delete the collection first\n",
    "if len(chroma_client.list_collections()) > 0 and collection_name in [chroma_client.list_collections()[0].name]:\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "\n",
    "print(f\"Creating collection: '{collection_name}'\")\n",
    "collection = chroma_client.create_collection(name=collection_name)\n",
    "\n",
    "# COMMAND ----------\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d9370d043c3a0eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 1: Add data to collection\n",
    "# MAGIC\n",
    "# MAGIC Since we are re-using the same data, we can skip the step of reading data. As mentioned in the text above, Chroma can take care of text vectorization for us, so we can directly add text to the collection and Chroma will convert the text into embeddings behind the scene. \n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "display(pdf_subset)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC Each document must have a unique `id` associated with it and it is up to you to check that there are no duplicate ids. \n",
    "# MAGIC\n",
    "# MAGIC Adding data to collection will take some time to run, especially when there is a lot of data. In the cell below, we intentionally write only a subset of data to the collection to speed things up. \n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "collection.add(\n",
    "    documents=pdf_subset[\"title\"][:100].tolist(),\n",
    "    metadatas=[{\"topic\": topic} for topic in pdf_subset[\"topic\"][:100].tolist()],\n",
    "    ids=[f\"id{x}\" for x in range(100)],\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ec5f5a42956667"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 2: Query for 10 relevant documents on \"space\"\n",
    "# MAGIC\n",
    "# MAGIC We will return 10 most relevant documents. You can think of `10` as 10 nearest neighbors. You can also change the number of results returned as well. \n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import json\n",
    "\n",
    "results = collection.query(query_texts=[\"space\"], n_results=10)\n",
    "\n",
    "print(json.dumps(results, indent=4))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Bonus: Add filter statement\n",
    "# MAGIC\n",
    "# MAGIC In addition to conducting relevancy search, we can also add filter statements. Refer to the [documentation](https://docs.trychroma.com/usage-guide#using-where-filters) for more information.\n",
    "\n",
    "# COMMAND ----------\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e5636548eb77172"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "collection.query(query_texts=[\"space\"], where={\"topic\": \"SCIENCE\"}, n_results=10)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Bonus: Update data in a collection\n",
    "# MAGIC\n",
    "# MAGIC Unlike a vector library, vector databases support changes to the data so we can update or delete data. \n",
    "# MAGIC\n",
    "# MAGIC Indeed, we can update or delete data in a Chroma collection. \n",
    "\n",
    "# COMMAND ----------\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a59795e64d32c64a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collection.delete(ids=[\"id0\"])\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC The record with `ids=0` is no longer present.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "collection.get(\n",
    "    ids=[\"id0\"],\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e638348a56c722e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC We can also update a specific data point.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "collection.get(\n",
    "    ids=[\"id2\"],\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "collection.update(\n",
    "    ids=[\"id2\"],\n",
    "    metadatas=[{\"topic\": \"TECHNOLOGY\"}],\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fef0878620dd5ee3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Prompt engineering for question answering \n",
    "# MAGIC\n",
    "# MAGIC Now that we have identified documents about space from the news dataset, we can pass these documents as additional context for a language model to generate a response based on them! \n",
    "# MAGIC\n",
    "# MAGIC We first need to pick a `text-generation` model. Below, we use a Hugging Face model. You can also use OpenAI as well, but you will need to get an Open AI token and [pay based on the number of tokens](https://openai.com/pricing). \n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=DA.paths.datasets)\n",
    "lm_model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir=DA.paths.datasets)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=lm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ecce37bdfc71492"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC Here's where prompt engineering, which is developing prompts, comes in. We pass in the context in our `prompt_template` but there are numerous ways to write a prompt. Some prompts may generate better results than the others and it requires some experimentation to figure out how best to talk to the model. Each language model behaves differently to prompts. \n",
    "# MAGIC\n",
    "# MAGIC Our prompt template below is inspired from a [2023 paper on program-aided language model](https://arxiv.org/pdf/2211.10435.pdf). The authors have provided their sample prompt template [here](https://github.com/reasoning-machines/pal/blob/main/pal/prompt/date_understanding_prompt.py).\n",
    "# MAGIC\n",
    "# MAGIC The following links also provide some helpful guidance on prompt engineering: \n",
    "# MAGIC - [Prompt engineering with OpenAI](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api)\n",
    "# MAGIC - [GitHub repo that compiles best practices to interact with ChatGPT](https://github.com/f/awesome-chatgpt-prompts)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "question = \"What's the latest news on space development?\"\n",
    "context = \" \".join([f\"#{str(i)}\" for i in results[\"documents\"][0]])\n",
    "prompt_template = f\"Relevant context: {context}\\n\\n The user's question: {question}\"\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "lm_response = pipe(prompt_template)\n",
    "print(lm_response[0][\"generated_text\"])\n",
    "\n",
    "# COMMAND ----------\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8b3e6305fbb437a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC\n",
    "# MAGIC Yay, you have just completed the implementation of your first text vectorization, search, and question answering workflow (that requires prompt engineering)!\n",
    "# MAGIC\n",
    "# MAGIC In the lab, you will apply your newly gained knowledge to a different dataset. You can also check out the optional modules on Pinecone and Weaviate to learn how to set up vector databases that offer enterprise offerings.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md-sandbox\n",
    "# MAGIC &copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "# MAGIC Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "# MAGIC <br/>\n",
    "# MAGIC <a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pdf = pd.read_csv(f\"{DA.paths.datasets}/news/labelled_newscatcher_dataset.csv\", sep=\";\")\n",
    "pdf[\"id\"] = pdf.index\n",
    "display(pdf)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "142223b222d19995"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
