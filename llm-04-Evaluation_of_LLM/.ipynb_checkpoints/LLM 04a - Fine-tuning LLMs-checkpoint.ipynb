{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a52b3a12-f644-4c5b-92f6-f60fb8fc5c74",
   "metadata": {},
   "source": [
    "# Databricks notebook source\n",
    " <div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "   <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec312f5d-a403-4f97-97e4-d192e393aebb",
   "metadata": {},
   "source": [
    " # Fine-tuning LLMs\n",
    "  Many LLMs are general purpose models trained on a broad range of data and use cases. This enables them to perform well in a variety of applications, as shown in previous modules. It is not uncommon though to find situations where applying a general purpose model performs unacceptably for specific dataset or use case. This often does not mean that the general purpose model is unusable. Perhaps, with some new data and additional training the model could be improved, or fine-tuned, such that it produces acceptable results for the specific use case.\n",
    "  \n",
    "  Fine-tuning uses a pre-trained model as a base and continues to train it with a new, task targeted dataset. Conceptually, fine-tuning leverages that which has already been learned by a model and aims to focus its learnings further for a specific task.\n",
    "\n",
    "  It is important to recognize that fine-tuning is model training. The training process remains a resource intensive, and time consuming effort. Albeit fine-tuning training time is greatly shortened as a result of having started from a pre-trained model. The model training process can be accelerated through the use of tools like Microsoft's [DeepSpeed](https://github.com/microsoft/DeepSpeed).\n",
    "\n",
    "  This notebook will explore how to perform fine-tuning at scale.\n",
    "\n",
    " ### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
    " 1. Prepare a novel dataset\n",
    " 1. Fine-tune the `t5-small` model to classify movie reviews.\n",
    " 1. Leverage DeepSpeed to enhance training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd2481-3c1f-4df4-83d8-05f8d788644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"gpu\" in spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\"), \"THIS LAB REQUIRES THAT A GPU MACHINE AND RUNTIME IS UTILIZED.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eed896-9055-4f6e-9530-7dca98dd4b40",
   "metadata": {},
   "source": [
    " ## Classroom Setup\n",
    " Later sections of this notebook will leverage the DeepSpeed package. DeepSpeed has some additional dependencies that need to be installed in the Databricks environment. The dependencies vary based upon which MLR runtime is being used. The below commands add the necessary libraries accordingly. It is convenient to perform this step at the start of the Notebook to avoid future restarts of the Python kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f8f081-38b5-4e33-8c32-57fbe74086c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac1e0e8-1495-434e-87c0-2540d18787ad",
   "metadata": {},
   "outputs": [],
   "source": [
    " # %sh\n",
    " # mkdir -p /tmp/externals/cuda\n",
    "\n",
    " # wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcurand-dev-11-7_10.2.10.50-1_amd64.deb -P /tmp/externals/cuda\n",
    " # wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcusparse-dev-11-7_11.7.3.50-1_amd64.deb -P /tmp/externals/cuda\n",
    " # wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcublas-dev-11-7_11.10.1.25-1_amd64.deb -P /tmp/externals/cuda\n",
    " # wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcusolver-dev-11-7_11.4.0.1-1_amd64.deb -P /tmp/externals/cuda\n",
    "\n",
    " # dpkg -i /tmp/externals/cuda/libcurand-dev-11-7_10.2.10.50-1_amd64.deb\n",
    " # dpkg -i /tmp/externals/cuda/libcusparse-dev-11-7_11.7.3.50-1_amd64.deb\n",
    " # dpkg -i /tmp/externals/cuda/libcublas-dev-11-7_11.10.1.25-1_amd64.deb\n",
    " # dpkg -i /tmp/externals/cuda/libcusolver-dev-11-7_11.4.0.1-1_amd64.deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c30092f-6e9d-43db-9eef-a7a85a7fe391",
   "metadata": {},
   "outputs": [],
   "source": [
    " %pip install deepspeed==0.9.1 py-cpuinfo==9.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f655fca3-0c53-41ad-8d65-4b4cdcc5e527",
   "metadata": {},
   "outputs": [],
   "source": [
    " %run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67e42795-ae0d-471c-860d-07caa90e9466",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (25381625.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    get_ipython().run_line_magic('load_ext', 'autoreload')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "\n",
    "\n",
    "\n",
    " %load_ext autoreload\n",
    " %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3f2cfc-1e47-43fa-b6ec-20c0cb4f0f78",
   "metadata": {},
   "source": [
    " Creating a local temporary directory on the Driver. This will serve as a root directory for the intermediate model checkpoints created during the training process. The final model will be persisted to DBFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2005ff8-f0e5-4a4a-b2fe-83f6f3ee2848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/tmpzux8klb4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "tmpdir = tempfile.TemporaryDirectory()\n",
    "local_training_root = tmpdir.name\n",
    "\n",
    "local_training_root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d696999e-d3f5-4283-8238-be08e1cd800c",
   "metadata": {},
   "source": [
    " ## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdf0b2b0-f2db-406f-abf8-740058c21425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import transformers as tr\n",
    "from datasets import load_dataset,load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd1bee4-f55b-4408-bcdf-d444a23fb6ea",
   "metadata": {},
   "source": [
    " ### Step 1 - Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b19814-ac2e-45d3-90ee-85a58ca58530",
   "metadata": {},
   "source": [
    " The first step of the fine-tuning process is to identify a specific task and supporting dataset. In this notebook, we will consider the specific task to be classifying movie reviews. This idea is generally simple task where a movie review is provided as plain-text and we would like to determine whether or not the review was positive or negative.\n",
    "\n",
    " The [IMDB dataset](https://huggingface.co/datasets/imdb) can be leveraged as a supporting dataset for this task. The dataset conveniently provides both a training and testing dataset with labeled binary sentiments, as well as a dataset of unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7af5e7d7-63d2-44fc-aebe-f150ffbf2ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_ds = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca738b25-c011-435b-a4e4-0e7e056944b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_ds = imdb_ds['train'].select(range(10))\n",
    "sample_test_ds = imdb_ds['test'].select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4ec99d5-c73e-41cc-b65a-686b68c37891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f8df1a-6758-486b-8065-c259b9de02e3",
   "metadata": {},
   "source": [
    " ### Step 2 - Select pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3b4798-ec66-4ad8-a5b2-375e38bf6ee2",
   "metadata": {},
   "source": [
    " The next step of the fine-tuning process is to select a pre-trained model. We will consider using the [T5](https://huggingface.co/docs/transformers/model_doc/t5) [[paper]](https://arxiv.org/pdf/1910.10683.pdf) family of models for our fine-tuning purposes. The T5 models are text-to-text transformers that have been trained on a multi-task mixture of unsupervised and supervised tasks. They are well suited for tasks such as summarization, translation, text classification, question answering, and more.\n",
    " \n",
    " * Recall from Module 1, Hugging Face provides the [Auto*](https://huggingface.co/docs/transformers/model_doc/auto) suite of objects to conveniently instantiate the various components associated with a pre-trained model. Here, we use the [AutoTokenizer](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer) to load in the tokenizer that is associated with the `t5-small` model.\n",
    " The `t5-small` version of the T5 models has 60 million parameters. This slimmed down version will be sufficient for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9276674-9eda-4f5d-ab0b-d247f75c9ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd99ed91-bc7a-4678-b88b-1142a7f8ee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer that was used for the t5-small model\n",
    "tokenizer = tr.AutoTokenizer.from_pretrained(\n",
    "    model_checkpoint\n",
    ")  # Use a pre-cached model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a360dbe-2e6c-42be-9289-7f51dc894a2d",
   "metadata": {},
   "source": [
    " As mentioned above, the IMDB dataset is a binary sentiment dataset. Its labels therefore are encoded as (-1 - unknown; 0 - negative; 1 - positive) values. In order to use this dataset with a text-to-text model like T5, the label set needs to be represented as a string. There are a number of ways to accomplish this. Here, we will simply translate each label id to its corresponding string value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bae784-cc9d-434b-9f58-a9c6ad0b1fd8",
   "metadata": {},
   "source": [
    "## Closure Function Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9ecca34-5048-4e4f-813f-ba140eb9d0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# closure Function Example Calls\n",
    "# its use for defining of pre-defined value instead of global variables\n",
    "def outer_function(x):\n",
    "    def inner_function(y):\n",
    "        return x + y  # inner_function has access to x from the outer scope\n",
    "    \n",
    "    return inner_function\n",
    "\n",
    "closure_instance = outer_function(10)  # closure_instance is now a closure with x=10\n",
    "output_outer_function= closure_instance(10)\n",
    "output_outer_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cbf52d2-08a8-4ba0-9178-ec0ee8ed39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tokens(\n",
    "    tokenizer: tr.models.t5.tokenization_t5_fast.T5TokenizerFast, label_map: dict\n",
    ") -> callable:\n",
    "    \"\"\"\n",
    "    Given a `tokenizer` this closure will iterate through `x` and return the result of `apply()`.\n",
    "    This function is mapped to a dataset and returned with ids and attention mask.\n",
    "    \"\"\"\n",
    "\n",
    "    def apply(x) -> tr.tokenization_utils_base.BatchEncoding:\n",
    "        \"\"\"From a formatted dataset `x` a batch encoding `token_res` is created.\"\"\"\n",
    "        \n",
    "        target_labels = [label_map[y] for y in x[\"label\"]]\n",
    "        print('target',target_labels)\n",
    "        \n",
    "        token_res = tokenizer(\n",
    "            x[\"text\"],\n",
    "            text_target=target_labels,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "        )\n",
    "        return token_res\n",
    "\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f5dba7d-6cec-4784-8f0a-95d228a98702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.to_tokens.<locals>.apply(x) -> transformers.tokenization_utils_base.BatchEncoding>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_label_lookup = {0: \"negative\", 1: \"positive\", -1: \"unknown\"}\n",
    "\n",
    "\n",
    "imdb_to_tokens = to_tokens(tokenizer, imdb_label_lookup)\n",
    "imdb_to_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c20a23e-d37d-4abe-99d3-6217ef79f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = sample_ds.map(\n",
    "    imdb_to_tokens, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6a793-f40d-458e-8864-39597d52daaf",
   "metadata": {},
   "source": [
    " ### Step 3 - Setup Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d82e2d-eca0-482d-84db-a5bafad32f63",
   "metadata": {},
   "source": [
    " The model training process is highly configurable. The [TrainingArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) class effectively exposes the configurable aspects of the process allowing one to customize them accordingly. Here, we will focus on setting up a training process that performs a single epoch of training with a batch size of 16. We will also leverage `adamw_torch` as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4540e7cc-c26f-4da8-b0fd-11a19b1e05da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/tmpzux8klb4/test-trainer'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_name = \"test-trainer\"\n",
    "\n",
    "local_checkpoint_path = os.path.join(local_training_root, checkpoint_name)\n",
    "local_checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8c540119-1c4f-40f1-959c-a4c60a2f29e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.36.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.28.2)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.26.3)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.26.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.7)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.18.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch!=1.12.0,>=1.10->transformers[torch]) (12.3.101)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (1.26.18)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9c35545-ec36-4ec2-9b23-93f902aa1d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = tr.TrainingArguments(\n",
    "    local_checkpoint_path,\n",
    "    num_train_epochs=1,  # default number of epochs to train is 3\n",
    "    per_device_train_batch_size=16,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=[\"tensorboard\"],\n",
    "    use_cpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b024eb-4ae3-47e8-a295-0c55cdb4f696",
   "metadata": {},
   "source": [
    " The pre-trained `t5-small` model can be loaded using the [AutoModelForSeq2SeqLM](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSeq2SeqLM) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db972dfc-4165-4240-9115-970b16b6d5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|███████████████████████████████████████████████████| 1.21k/1.21k [00:00<00:00, 1.86MB/s]\n",
      "model.safetensors: 100%|███████████████████████████████████████████████| 242M/242M [01:28<00:00, 2.74MB/s]\n",
      "generation_config.json: 100%|█████████████████████████████████████████████| 147/147 [00:00<00:00, 232kB/s]\n"
     ]
    }
   ],
   "source": [
    "# load the pre-trained model\n",
    "model = tr.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_checkpoint, \n",
    ")  # Use a pre-cached model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80b3b632-3234-49cb-ac54-3b914cc8a684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 10:23:59.380111: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-17 10:23:59.380387: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-17 10:23:59.421841: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-17 10:23:59.537282: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-17 10:24:00.753721: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column train not in the dataset. Current columns in the dataset: ['input_ids', 'attention_mask', 'labels']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# used to assist the trainer in batching the data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m tr\u001b[38;5;241m.\u001b[39mDataCollatorWithPadding(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m tr\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      4\u001b[0m     model,\n\u001b[1;32m      5\u001b[0m     training_args,\n\u001b[0;32m----> 6\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m      7\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      8\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m      9\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:2800\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2799\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:2784\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2782\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   2783\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2784\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2785\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2786\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2787\u001b[0m )\n\u001b[1;32m   2788\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py:580\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    578\u001b[0m     _raise_bad_key_type(key)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 580\u001b[0m     \u001b[43m_check_valid_column_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py:520\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[0;34m(key, columns)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_valid_column_key\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, columns: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[0;32m--> 520\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column train not in the dataset. Current columns in the dataset: ['input_ids', 'attention_mask', 'labels']\""
     ]
    }
   ],
   "source": [
    "# used to assist the trainer in batching the data\n",
    "data_collator = tr.DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainer = tr.Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e337f1f7-6c5d-4278-bbf9-7ad8924e0bbb",
   "metadata": {},
   "source": [
    " ### Step 4 - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344d6d2b-84e8-4625-a098-eca08e6f25dc",
   "metadata": {},
   "outputs": [],
   "source": [
    " Before starting the training process, let's turn on Tensorboard. This will allow us to monitor the training process as checkpoint logs are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1e5d03-63e3-480a-92e3-8b707a29b3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_display_dir = f\"{local_checkpoint_path}/runs\"\n",
    "\n",
    "\n",
    "\n",
    " %load_ext tensorboard\n",
    " %tensorboard --logdir '{tensorboard_display_dir}'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " Start the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58bc12f-594e-4cf0-8499-33c195a690e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "# save model to the local checkpoint\n",
    "trainer.save_model()\n",
    "trainer.save_state()\n",
    "\n",
    "\n",
    "\n",
    "# persist the fine-tuned model to DBFS\n",
    "final_model_path = f\"{DA.paths.working_dir}/llm04_fine_tuning/{checkpoint_name}\"\n",
    "trainer.save_model(output_dir=final_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe8f5e4-c67d-4003-926b-d06e24d3a2a4",
   "metadata": {},
   "source": [
    " ### Step 5 - Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c9fb8-df76-4e96-ac09-c32d86e559c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = tr.AutoModelForSeq2SeqLM.from_pretrained(final_model_path)\n",
    "\n",
    "\n",
    "\n",
    "reviews = [\n",
    "    \"\"\"\n",
    "'Despicable Me' is a cute and funny movie, but the plot is predictable and the characters are not very well-developed. Overall, it's a good movie for kids, but adults might find it a bit boring.\"\"\",\n",
    "    \"\"\" 'The Batman' is a dark and gritty take on the Caped Crusader, starring Robert Pattinson as Bruce Wayne. The film is a well-made crime thriller with strong performances and visuals, but it may be too slow-paced and violent for some viewers.\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "The Phantom Menace is a visually stunning film with some great action sequences, but the plot is slow-paced and the dialogue is often wooden. It is a mixed bag that will appeal to some fans of the Star Wars franchise, but may disappoint others.\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "I'm not sure if The Matrix and the two sequels were meant to have a tigh consistency but I don't think they quite fit together. They seem to have a reasonably solid arc but the features from the first aren't in the second and third as much, instead the second and third focus more on CGI battles and more visuals. I like them but for different reasons, so if I'm supposed to rate the trilogy I'm not sure what to say.\n",
    "\"\"\",\n",
    "]\n",
    "inputs = tokenizer(reviews, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "pred = fine_tuned_model.generate(\n",
    "    input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "pdf = pd.DataFrame(\n",
    "    zip(reviews, tokenizer.batch_decode(pred, skip_special_tokens=True)),\n",
    "    columns=[\"review\", \"classification\"],\n",
    ")\n",
    "display(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54541c66-94f0-406b-a93d-92241bb20559",
   "metadata": {},
   "source": [
    " ## DeepSpeed\n",
    "\n",
    " As model architectures evolve and grow, they continually push the limits of available computational resources. For example, some large LLMs having hundreds of billions of parameters making them too large to fit, in some cases, in available GPU memory. Models of this scale therefore need to leverage distributed processing or high-end hardware, and sometimes even both, to support training efforts. This makes large model training a costly undertaking, and therefore accelerating the training process is highly desirable.\n",
    "\n",
    " As mentioned above, one such framework that can be leveraged to accelerate the model training process is Microsoft's [DeepSpeed](https://github.com/microsoft/DeepSpeed) [[paper]](https://arxiv.org/pdf/2207.00032.pdf). This framework provides advances in compression, distributed training, mixed precision, gradient accumulation, and checkpointing.\n",
    "\n",
    " It is worth noting that DeepSpeed is intended for large models that do not fit into device memory. The `t5-base` model we are using is not a large model, and therefore DeepSpeed is not expected to provide a benefit.\n",
    "\n",
    " ### !! Please do not attempt this in Vocareum as it will take more than 5 hours to run and exhaust your compute budget!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " ### Environment Setup\n",
    "\n",
    " The intended use for DeepSpeed is in a distributed compute environment. As such, each node of the environment is assigned a `rank` and `local_rank` in relation to the size of the distributed environment.\n",
    "\n",
    " Here, since we are testing with a single node/GPU environment we will set the `world_size` to 1, and both `ranks` to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d483297d-cd5b-489a-8b4b-0af285cdf1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"9994\"  # modify if RuntimeError: Address already in use\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e82c05-9c58-4abc-b19b-6fcbecaca15f",
   "metadata": {},
   "source": [
    " ### Configuration\n",
    "\n",
    " There are a number of [configuration options](https://www.deepspeed.ai/docs/config-json/) that can be set to enhance the training and inference processes. The [ZeRO optimization](https://www.deepspeed.ai/training/#memory-efficiency) settings target reducing the memory footprint allowing for larger models to be efficiently trained on limited resources. \n",
    "\n",
    " The Hugging Face `TrainerArguments` accept the configuration either from a JSON file or a dictionary. Here, we will define the dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5b61a-fe52-4d46-9283-7df92c446287",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_config = {\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"offload_optimizer\": {\"device\": \"cpu\", \"pin_memory\": True},\n",
    "        \"allgather_partitions\": True,\n",
    "        \"allgather_bucket_size\": 5e8,\n",
    "        \"overlap_comm\": True,\n",
    "        \"reduce_scatter\": True,\n",
    "        \"reduce_bucket_size\": 5e8,\n",
    "        \"contiguous_gradients\": True,\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"betas\": \"auto\",\n",
    "            \"eps\": \"auto\",\n",
    "            \"weight_decay\": \"auto\",\n",
    "            \"torch_adam\": True,\n",
    "        },\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_min_lr\": \"auto\",\n",
    "            \"warmup_max_lr\": \"auto\",\n",
    "            \"warmup_num_steps\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "model_checkpoint = \"t5-base\"\n",
    "tokenizer = tr.AutoTokenizer.from_pretrained(\n",
    "    model_checkpoint, cache_dir=DA.paths.datasets\n",
    ")\n",
    "\n",
    "imdb_to_tokens = to_tokens(tokenizer, imdb_label_lookup)\n",
    "tokenized_dataset = imdb_ds.map(\n",
    "    imdb_to_tokens, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "\n",
    "model = tr.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_checkpoint, cache_dir=DA.paths.datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866ecab6-d85a-40bb-b831-b76c34253f1e",
   "metadata": {},
   "source": [
    " ### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd087fc-52ba-4906-8fe0-fa21ec3dbfce",
   "metadata": {},
   "outputs": [],
   "source": [
    " There are only two changes made to the training setup from above. The first is to set a new checkpoint name. The second is to add the `deepspeed` configuration to the `TrainingArguments`.\n",
    "\n",
    " Note: at this time the `deepspeed` argument is considered an experimental feature and may evolve in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a9d65-0c83-4d2e-9357-b76542ea78a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = \"test-trainer-deepspeed\"\n",
    "checkpoint_location = os.path.join(local_training_root, checkpoint_name)\n",
    "training_args = tr.TrainingArguments(\n",
    "    checkpoint_location,\n",
    "    num_train_epochs=3,  # default number of epochs to train is 3\n",
    "    per_device_train_batch_size=8,\n",
    "    deepspeed=zero_config,  # add the deepspeed configuration\n",
    "    report_to=[\"tensorboard\"],\n",
    ")\n",
    "\n",
    "data_collator = tr.DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainer = tr.Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "tensorboard_display_dir = f\"{checkpoint_location}/runs\"\n",
    "\n",
    "\n",
    "\n",
    " %load_ext tensorboard\n",
    " %tensorboard --logdir '{tensorboard_display_dir}'\n",
    "\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model()\n",
    "trainer.save_state()\n",
    "\n",
    "\n",
    "\n",
    "# persist the fine-tuned model to DBFS\n",
    "final_model_path = f\"{DA.paths.working_dir}/llm04_fine_tuning/{checkpoint_name}\"\n",
    "trainer.save_model(output_dir=final_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22c407e-c23e-4bcc-9841-fa7aa4f4a6ba",
   "metadata": {},
   "source": [
    " ### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfa8259-b4c1-45b7-a680-dde569381228",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = tr.AutoModelForSeq2SeqLM.from_pretrained(final_model_path)\n",
    "\n",
    "\n",
    "\n",
    "review = [\n",
    "    \"\"\"\n",
    "           I'm not sure if The Matrix and the two sequels were meant to have a tight consistency but I don't think they quite fit together. They seem to have a reasonably solid arc but the features from the first aren't in the second and third as much, instead the second and third focus more on CGI battles and more visuals. I like them but for different reasons, so if I'm supposed to rate the trilogy I'm not sure what to say.\"\"\"\n",
    "]\n",
    "inputs = tokenizer(review, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "pred = fine_tuned_model.generate(\n",
    "    input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "pdf = pd.DataFrame(\n",
    "    zip(review, tokenizer.batch_decode(pred, skip_special_tokens=True)),\n",
    "    columns=[\"review\", \"classification\"],\n",
    ")\n",
    "display(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2900d2bb-c35f-4f9c-bc35-0007a08a5f53",
   "metadata": {},
   "source": [
    " ## Clean up Classroom\n",
    "\n",
    " Run the following cell to remove lessons-specific assets created during this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a064465-c629-424b-8613-e44f6f518937",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cc1fc3-3b55-46ee-bb53-eefeca62c3fc",
   "metadata": {},
   "source": [
    " &copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    " Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    " <br/>\n",
    " <a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31be6a1d-b14e-4d8a-86c7-4c7b80552893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
